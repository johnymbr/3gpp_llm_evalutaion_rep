{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2027ee1a-a3a3-4517-a2df-8b2fa3da3f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  2.5.0+cu124\n",
      "Torch Version Cuda:  12.4\n",
      "Torch Version cuDnn:  90800\n",
      "CUDA Available:  True\n",
      "CUDA Device Name:  NVIDIA RTX A1000 6GB Laptop GPU\n",
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch Version: \", torch.__version__)\n",
    "print(\"Torch Version Cuda: \", torch.version.cuda)\n",
    "print(\"Torch Version cuDnn: \", torch.backends.cudnn.version())\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "print(\"CUDA Device Name: \", torch.cuda.get_device_name(0))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verificar se CUDA estÃ¡ disponÃ­vel para acelerar o processamento\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48330ffa-82b1-48d6-bbd0-5837f42c263d",
   "metadata": {},
   "source": [
    "# Installing unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0c863-525f-480f-91b1-3ebe82e5e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it was installed in our venv...\n",
    "# pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68de254-2224-4a92-8eae-b450e1287160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5155b085-a9ca-40d8-8e17-f5272d9a44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c286b4e4-75e5-45d4-95a3-f8fc80193d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Llama patching. Transformers: 4.50.0.\n",
      "   \\\\   /|    NVIDIA RTX A1000 6GB Laptop GPU. Num GPUs = 1. Max memory: 5.681 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4f008b427c4f559a3e1d6edbe0f4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106328be9d704d78a027634a059d73ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffeb19e1f3ea48839f4b2140dc3f1b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47212e4ccff143bd98b4a9f391a874b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b6f002034e49c78b3ab8a4d38aa5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name=\"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\", # or choose \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # device_map=\"auto\"\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = initial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7424c352-84da-42a4-a284-846b27f669f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to CPU before delete\n",
    "# model.to(\"cpu\")\n",
    "# delete reference to the model\n",
    "# del model\n",
    "\n",
    "# import gc and call garbage collect to free orphan objects\n",
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    "# Free GPU memor\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b137c634-ece2-4fc8-9bbd-701a6e60ef98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50672555-bde6-4268-9e01-ab7493a75136",
   "metadata": {},
   "source": [
    "# Dataset TeleQnA\n",
    "\n",
    "We will load json files that we generated and create dataset to fine-tunning the model.\n",
    "\n",
    "## Questions Release 17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1571e6a4-d41d-4082-b0f6-b343a6f974c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to the TeleQnA processed question in JSON file with only Rel 17...\n",
    "rel17_questions_path = r\"../files/rel17_questions.json\"\n",
    "\n",
    "# Load the TeleQnA data just release 17\n",
    "with open(rel17_questions_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    rel17_questions = json.load(file)\n",
    "print(len(rel17_questions))\n",
    "\n",
    "# Path to the TeleQnA processed question in JSON file with only Rel 17 and 100 questions...\n",
    "rel17_100_questions_path = r\"../files/rel17_100_questions.json\"\n",
    "\n",
    "# Load the TeleQnA data just release 17\n",
    "with open(rel17_200_questions_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    rel17_200_questions = json.load(file)\n",
    "print(len(rel17_200_questions))\n",
    "\n",
    "rel17_other_questions = [q for q in rel17_questions if q not in rel17_100_questions]\n",
    "print(len(rel17_other_questions))\n",
    "\n",
    "rel17_other_questions_length = 500\n",
    "rel17_other_questions = rel17_other_questions[:rel17_other_questions_length]\n",
    "rel17_other_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451eafc-8752-4434-926f-df5a6e3466b0",
   "metadata": {},
   "source": [
    "## Questions without Rel 17 and 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e786b2a-3ccf-4376-b2cc-fc2d65545fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the TeleQnA processed question in JSON file with questions without Rel 17 and 18...\n",
    "no_rel_17_18_path_questions = r\"../files/no_rel_17_18_questions.json\"\n",
    "\n",
    "# Load the TeleQnA data...\n",
    "with open(no_rel_17_18_path_questions, \"r\", encoding=\"utf-8\") as file:\n",
    "    no_rel_17_18_questions = json.load(file)\n",
    "print(len(no_rel_17_18_questions))\n",
    "\n",
    "no_rel_17_18_questions_length = 3500\n",
    "no_rel_17_18_questions = no_rel_17_18_questions[:questions_no_rel_17_18_length]\n",
    "print(len(no_rel_17_18_questions))\n",
    "no_rel_17_18_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142694ed-7231-4801-9b03-c4b3136c3f06",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5fd1c-bd33-40b9-bed9-256fa05022e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions = rel17_other_questions + no_rel_17_18_questions\n",
    "print(len(train_questions))\n",
    "train_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe139cc-5eac-453a-adc9-61032c90c984",
   "metadata": {},
   "source": [
    "We create two datasets, one with no options and half of questions and another wit options and the other half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a90901-6f76-4d5e-9cbe-8bc72a850564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Structure to store pairs of questions and explanations\n",
    "data = []\n",
    "\n",
    "half_questions = len(train_questions)//2\n",
    "\n",
    "# Fill the dataset with (question, explanation) pairs\n",
    "for item in train_questions[:half_questions]:\n",
    "\n",
    "    human_value = (\n",
    "        f\"{item['question']}\"\n",
    "    )\n",
    "\n",
    "    # Combine the answer and explanation\n",
    "    gpt_value = (\n",
    "        f\"{item['explanation']}\"\n",
    "    )\n",
    "\n",
    "    # Create a dictionary for each input pair\n",
    "    pair = [\n",
    "        {'from': 'human', 'value': human_value},  # For the question\n",
    "        {'from': 'gpt', 'value': gpt_value}  # For the explanation\n",
    "    ]\n",
    "\n",
    "    data.append(pair)  # Add the pair to the dataset\n",
    "\n",
    "data_no_options = data\n",
    "print(data_no_options[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6771965-3cd2-4289-b621-6ec95a364958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Structure to store pairs of questions, options, answers, and explanations\n",
    "data = []\n",
    "\n",
    "# Fill the dataset with (question + options, answer + explanation) pairs\n",
    "for item in train_questions[half_questions:]:\n",
    "\n",
    "    # Extract options\n",
    "    options = [f\"{key}: {value}\" for key, value in item.items() if 'option' in key]\n",
    "    # Combine the question and options\n",
    "    human_value = (\n",
    "        f\"Question: {item['question']}\\n\"\n",
    "        f\"Options:\\n\" + \"\\n\".join(options) + \"\\n\"\n",
    "    )\n",
    "\n",
    "    # Combine the answer and explanation\n",
    "    gpt_value = (\n",
    "        f\"Answer: {item['answer']}\\n\"\n",
    "        f\"Explanation: {item['explanation']}\"\n",
    "    )\n",
    "\n",
    "    # Create a dictionary for each input pair\n",
    "    pair = [\n",
    "        {'from': 'human', 'value': human_value},  # Question with options\n",
    "        {'from': 'gpt', 'value': gpt_value}       # Answer with explanation\n",
    "    ]\n",
    "\n",
    "    data.append(pair)  # Add the pair to the dataset\n",
    "\n",
    "# Create the dataset using Hugging Face\n",
    "data_options = data\n",
    "print(data_options[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde7707-ad95-4688-a7b9-8f139bdf6198",
   "metadata": {},
   "source": [
    "Then we join these datasets and shuffle randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76e822-2208-4ad9-b1b5-7a23c521e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data_total = data_no_options + data_options\n",
    "# Shuffle the combined data\n",
    "random.shuffle(data_total)\n",
    "print(len(data_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c8f37-c68f-4933-8f6e-e2da944f17a2",
   "metadata": {},
   "source": [
    "Convert the list of pairs into the appropriate format\n",
    "Transform the data into a Dataset\n",
    "Save dataset on the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36eb8d2-b55e-4de0-b1af-61545923762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = {'conversations': data_total}\n",
    "dataset = Dataset.from_dict(formatted_data)\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[0])\n",
    "\n",
    "dataset.save_to_disk('../files/train_questions_dataset_4000_questions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df992f6b-52f2-4921-997f-a6af0a9789d4",
   "metadata": {},
   "source": [
    "In the next time that we run our model it isn't necessary to recreate dataset, just load it from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae48d26-b594-412d-9144-1dd7a9d937e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_path = '../files/train_questions_dataset_4000_questions'\n",
    "\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "print(len(dataset))\n",
    "dataset[0]\n",
    "dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7566c4-96e4-4d54-826f-c49887e0b6ed",
   "metadata": {},
   "source": [
    "# Format TeleQnA Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a64447-bea5-4746-8010-ecb62062abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "dataset\n",
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dac969-5e22-4b63-bef4-22132cc1e445",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8aa41d-7155-4831-a2bd-3d6814eece51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "# if necessary run command below to delete model\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a2e769-44d8-494f-92ac-7f1e9b4a3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 16,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 300,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0a643-5a1c-4a90-9c71-b654702496ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1eb76b-86bf-44f3-9326-ed7eef38173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72ebe1-4305-4748-ad31-5c2ebd867efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852cb90-8a6d-496e-b14f-0c784f588609",
   "metadata": {},
   "source": [
    "# Show current memory stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13e935-4b0c-480d-aa9e-93e50b281dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8e6cd-07e2-48c8-937d-8e8523dc9018",
   "metadata": {},
   "source": [
    "# Initial Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d036f-1761-4077-86ec-a7a7fe37945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_dataset[0].items()\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configura o collator e DataLoader\n",
    "collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "loader = DataLoader(trainer.train_dataset,\n",
    "                    batch_size=2,  # Tamanho do batch escolhido\n",
    "                    collate_fn=collator,\n",
    "                    num_workers=2)\n",
    "\n",
    "# VariÃ¡veis para armazenar a loss total e o nÃºmero de exemplos\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "# Coloca o modelo em modo de avaliaÃ§Ã£o\n",
    "model.eval()\n",
    "\n",
    "# Desativa o cÃ¡lculo de gradiente para economizar memÃ³ria\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Calculating initial loss\"):\n",
    "        # Move o batch para a GPU (se disponÃ­vel)\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # Acumula a loss\n",
    "        total_loss += outputs.loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "# Calcula a loss mÃ©dia\n",
    "average_loss = total_loss / num_batches\n",
    "print(f\"Initial mean loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a4635-281b-4ac3-bced-ee038d0286c1",
   "metadata": {},
   "source": [
    "# Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff767b87-c762-482f-bbfa-8dbe2bd16c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a7b28-83d3-44c1-aaa4-6850cadf1e14",
   "metadata": {},
   "source": [
    "# Show final memory and time stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71192c-f00f-43c9-a49f-8776e1b128de",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fdeaf-0bfc-482e-ad46-3912c440ee67",
   "metadata": {},
   "source": [
    "# Loss Pos training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c10f7-936d-4943-a097-786e9a746a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure the collator and DataLoader\n",
    "collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "loader = DataLoader(\n",
    "    trainer.train_dataset,\n",
    "    batch_size=2,  # Chosen batch size\n",
    "    collate_fn=collator,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Variables to store the total loss and the number of valid batches\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation to save memory\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Calculating loss post-training\"):\n",
    "        # Move the batch to GPU (if available)\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # Check if the loss is NaN\n",
    "        if not torch.isnan(outputs.loss):\n",
    "            total_loss += outputs.loss.item()\n",
    "            num_batches += 1\n",
    "            # print(outputs.loss.item())\n",
    "\n",
    "print(f\"Number of valid batches: {num_batches}\")\n",
    "\n",
    "# Calculate the average loss\n",
    "if num_batches > 0:\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Post-training mean loss: {average_loss}\")\n",
    "else:\n",
    "    print(\"No valid batches found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca1f88f-62ab-4c1c-bf30-489661fb62f3",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e37291-a4a9-4816-ab76-3b306c233676",
   "metadata": {},
   "source": [
    "## Question with option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370ca64-40e2-42c4-bfc6-60360f72dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[5]['conversations'][0]\n",
    "question = dataset[5]['conversations'][0]['value']\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"user\", \"content\": \"How much is 1+1?\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca16f94-19a2-4330-98da-1a9c5a1af9fb",
   "metadata": {},
   "source": [
    "## Question with no option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219ef6a-64ee-49d2-9083-55c1df010a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['conversations'][0]['value']\n",
    "question = dataset[0]['conversations'][0]['value']\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"user\", \"content\": \"How much is 1+1?\"},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f1ed5c-e544-460e-889a-936c4e908563",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c0380a-e46e-4a19-8052-423bde66f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../models/llama_3.2_1B_FT_lora_4000_questions\", safe_serialization=False)\n",
    "tokenizer.save_pretrained(\"../models/llama_3.2_1B_FT_lora_4000_questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82326ed-5ec8-4f15-8bb7-94bbb3e8c388",
   "metadata": {},
   "source": [
    "## Save merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7d1d1-b754-471c-8f8d-8e2a13b00942",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"../models/llama_3.2_1B_FT_lora_4bits_4000_questions\", tokenizer, save_method = \"merged_4bit\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f704080-0a03-4b07-b014-9c144652122b",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861668a-f867-4358-8bbd-2d68e697bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2c0f4-5e95-4a40-a937-dad5d7d60d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/llama_3.2_1B_FT_lora_4000_questions\"\n",
    "# model_path = \"model_3.2_lora_4bits\"\n",
    "\n",
    "# Carregar o modelo e o tokenizador separadamente\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
