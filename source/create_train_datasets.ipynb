{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b0ddb3-c890-424b-a860-8ca06bd4aa8c",
   "metadata": {},
   "source": [
    "# Process the TeleQnA Dataset to create and save train datasets\n",
    "\n",
    "Processing TeleQnA dataset to produces datasets to fine tunne a model and then test it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3de1b-8306-4195-81fe-d485f4015183",
   "metadata": {},
   "source": [
    "# Dataset TeleQnA\n",
    "We will load json files that we generated and create dataset to fine-tunning the model.\n",
    "\n",
    "## Questions Release 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8539f8-f071-4ad6-b4f1-72e6a02a8ca9",
   "metadata": {},
   "source": [
    "## Dataset with 4000 questions\n",
    "\n",
    "### Questions Release 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383fa04b-7cc0-40a1-a0fe-0ccff4147667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to the TeleQnA processed question in JSON file with only Rel 17...\n",
    "rel17_questions_path = r\"../files/rel17_questions.json\"\n",
    "\n",
    "# Load the TeleQnA data just release 17\n",
    "with open(rel17_questions_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    rel17_questions = json.load(file)\n",
    "print(len(rel17_questions))\n",
    "\n",
    "# Path to the TeleQnA processed question in JSON file with only Rel 17 and 100 questions...\n",
    "rel17_100_questions_path = r\"../files/rel17_100_questions.json\"\n",
    "\n",
    "# Load the TeleQnA data just release 17\n",
    "with open(rel17_100_questions_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    rel17_100_questions = json.load(file)\n",
    "print(len(rel17_100_questions))\n",
    "\n",
    "rel17_other_questions = [q for q in rel17_questions if q not in rel17_100_questions]\n",
    "print(len(rel17_other_questions))\n",
    "\n",
    "rel17_other_questions_length = 500\n",
    "rel17_other_questions = rel17_other_questions[:rel17_other_questions_length]\n",
    "print(len(rel17_other_questions))\n",
    "rel17_other_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c853bbd1-1f24-4f12-a7f9-79cbbc627e97",
   "metadata": {},
   "source": [
    "### Questions without Rel 17 and 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e61cc-dfaa-44d1-aec4-4fe7469eadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the TeleQnA processed question in JSON file with questions without Rel 17 and 18...\n",
    "no_rel_17_18_path_questions = r\"../files/no_rel_17_18_questions.json\"\n",
    "\n",
    "# Load the TeleQnA data...\n",
    "with open(no_rel_17_18_path_questions, \"r\", encoding=\"utf-8\") as file:\n",
    "    no_rel_17_18_questions = json.load(file)\n",
    "print(len(no_rel_17_18_questions))\n",
    "\n",
    "no_rel_17_18_questions_length = 3500\n",
    "no_rel_17_18_questions = no_rel_17_18_questions[:no_rel_17_18_questions_length]\n",
    "print(len(no_rel_17_18_questions))\n",
    "no_rel_17_18_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f0f7e-2492-4795-9fe4-4574bfdfcf93",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df94fd4-a9d0-472f-8c55-39ca3b3a2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions = rel17_other_questions + no_rel_17_18_questions\n",
    "print(len(train_questions))\n",
    "train_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cebb8f9-269c-4be3-b9ba-ef818e1413f9",
   "metadata": {},
   "source": [
    "We create two datasets, one with no options and half of questions and another wit options and the other half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd927d-8c6f-4011-8da4-e4d0b958dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Structure to store pairs of questions and explanations\n",
    "data = []\n",
    "\n",
    "half_questions = len(train_questions)//2\n",
    "\n",
    "# Fill the dataset with (question, explanation) pairs\n",
    "for item in train_questions[:half_questions]:\n",
    "\n",
    "    human_value = (\n",
    "        f\"{item['question']}\"\n",
    "    )\n",
    "\n",
    "    # Combine the answer and explanation\n",
    "    gpt_value = (\n",
    "        f\"{item['explanation']}\"\n",
    "    )\n",
    "\n",
    "    # Create a dictionary for each input pair\n",
    "    pair = [\n",
    "        {'from': 'human', 'value': human_value},  # For the question\n",
    "        {'from': 'gpt', 'value': gpt_value}  # For the explanation\n",
    "    ]\n",
    "\n",
    "    data.append(pair)  # Add the pair to the dataset\n",
    "\n",
    "data_no_options = data\n",
    "print(data_no_options[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72646d45-e3f2-450f-9952-43c91b3cc9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Structure to store pairs of questions, options, answers, and explanations\n",
    "data = []\n",
    "\n",
    "# Fill the dataset with (question + options, answer + explanation) pairs\n",
    "for item in train_questions[half_questions:]:\n",
    "\n",
    "    # Extract options\n",
    "    options = [f\"{key}: {value}\" for key, value in item.items() if 'option' in key]\n",
    "    # Combine the question and options\n",
    "    human_value = (\n",
    "        f\"Question: {item['question']}\\n\"\n",
    "        f\"Options:\\n\" + \"\\n\".join(options) + \"\\n\"\n",
    "    )\n",
    "\n",
    "    # Combine the answer and explanation\n",
    "    gpt_value = (\n",
    "        f\"Answer: {item['answer']}\\n\"\n",
    "        f\"Explanation: {item['explanation']}\"\n",
    "    )\n",
    "\n",
    "    # Create a dictionary for each input pair\n",
    "    pair = [\n",
    "        {'from': 'human', 'value': human_value},  # Question with options\n",
    "        {'from': 'gpt', 'value': gpt_value}       # Answer with explanation\n",
    "    ]\n",
    "\n",
    "    data.append(pair)  # Add the pair to the dataset\n",
    "\n",
    "# Create the dataset using Hugging Face\n",
    "data_options = data\n",
    "print(data_options[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36666509-ed70-403a-8b85-b60b051a81ba",
   "metadata": {},
   "source": [
    "Then we join these datasets and shuffle randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb1457-fddc-4671-9dd2-34bdca11300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data_total = data_no_options + data_options\n",
    "# Shuffle the combined data\n",
    "random.shuffle(data_total)\n",
    "print(len(data_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54f11b-b237-4d4f-b77c-918ceecc1be0",
   "metadata": {},
   "source": [
    "Convert the list of pairs into the appropriate format Transform the data into a Dataset Save dataset on the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ee1fa-a869-4033-9360-031991508492",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = {'conversations': data_total}\n",
    "dataset = Dataset.from_dict(formatted_data)\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[0])\n",
    "\n",
    "dataset.save_to_disk('../files/train_questions_dataset_4000_questions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c425e99-6cde-40b2-ba31-c326220ec0b1",
   "metadata": {},
   "source": [
    "## Dataset with 4000 questions: answer label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c6540-31fb-420d-b3e4-85f2cff722c2",
   "metadata": {},
   "source": [
    "## Dataset with 4000 questions: short answer label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
