import torch
import json
import re
import string
from pathlib import Path
from unsloth import FastLanguageModel
from unsloth.chat_templates import get_chat_template
from rag_functions import load_faiss_index, search_faiss_index, search_rag, load_chunks


def extract_option(text, first_option=True):
    """
    Extract the option part from the answer string, removing all punctuation and converting to lowercase.

    Parameters:
    - answer: A string containing the answer in the format 'option X: ...'.

    Returns:
    - String: Extracted option (e.g., 'option 2'), or None if no match is found.
    """
    if first_option:
        # Remove all punctuation and convert to lowercase
        cleaned_answer = re.sub(r'[^\w\s]', '', text.lower())
        # Find all matches for the format "option X"
        matches = re.findall(r'option \d+', cleaned_answer)
        # Return the last match with stripped whitespace if any found, otherwise None
        return matches[-1].strip() if matches else None
    else:
        # Find all occurrences of 'option X:' followed by text, where X can be any number
        option_matches = re.findall(r'option\s*\d+:\s*(.*)', text, re.IGNORECASE | re.DOTALL)

        # Return the text after the last 'option X:' found
        return option_matches[-1].strip() if option_matches else None


def extract_response_after_assistant(response):
    """
    Extract the part of the response that comes after the 'assistant' marker.

    Parameters:
    - response: The complete response from the model.

    Returns:
    - String: The extracted relevant part of the response.
    """
    # Split the response based on the 'assistant' marker
    parts = response.split('assistant', 1)
    # Return the part after 'assistant' or the entire response if 'assistant' is not found
    return parts[1].strip() if len(parts) > 1 else response.strip()


def evaluate_model_response(model_response, question_data, first_option=True):
    """
    Compare the model's response with the correct answer from the question data.

    Parameters:
    - model_response: The response string generated by the model.
    - question_data: Dictionary containing the question, options, and the correct answer.

    Returns:
    - 1 if the response is correct, otherwise the extracted model option.
    """
    correct_option = extract_option(question_data['answer'], first_option=first_option)  # Extract correct option
    relevant_response = extract_response_after_assistant(model_response)  # Get relevant part of response
    model_option = extract_option(relevant_response, first_option=first_option)  # Extract model's option

    return 1 if model_option == correct_option else model_option  # Return 1 if correct, else model's option


def format_answer(answer):
    # Remove punctuation and convert to lowercase
    answer_no_punctuation = answer.translate(str.maketrans('', '', string.punctuation))
    return answer_no_punctuation.lower()


def extract_answer(text):
    # Check for the presence of 'assistant'
    assistant_match = re.search(r'assistant\s*(.*)', text, re.IGNORECASE | re.DOTALL)
    if assistant_match:
        # If 'assistant' is found, get the text that follows it
        assistant_text = assistant_match.group(1).strip()

        # Find all occurrences of 'answer:' and capture the text after the last one
        answer_matches = re.findall(r'answer:\s*(.*)', assistant_text, re.IGNORECASE | re.DOTALL)

        # Return the phrase after the last 'answer:' found
        return answer_matches[-1].strip() if answer_matches else assistant_text

    # Return None if 'assistant' is not found
    return None


class InferenceRagLlama:
    def __init__(self, model_path: str, chunks_path: str, faiss_path: str, question_path: str, output_path: str):
        self.model = None
        self.tokenizer = None
        self.questions = None

        self.output_path = output_path

        self.model_path = model_path
        self.chunks_path = chunks_path
        self.faiss_path = faiss_path
        self.questions_path = question_path
        self.max_seq_length = 4096
        self.dtype = None
        self.load_in_4bit = True  # Use 4bit quantization to reducer memory usage.

    def load_questions_mount_model_tokenizer(self):
        print("**********\nLoad Questions and Mount Model and Tokenizer\n**********")
        with open(self.questions_path, "r", encoding="utf-8") as file:
            self.questions = json.load(file)

        print(f"Questions size: {len(self.questions)}")
        print(self.questions[0])

        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.model_path,
            max_seq_length=self.max_seq_length,
            dtype=self.dtype,
            load_in_4bit=self.load_in_4bit
        )

        print(self.model)

    def ask_llama_3_2_lora_rag(self, question_data, top_k=5):
        """
        Function to generate an answer using the model based on the given question and options,
        including relevant information from a RAG search and prompting a chain of thought.

        Parameters:
        - question_data: Dictionary containing the question and options.
        - top_k: Number of relevant chunks to retrieve from the search.

        Returns:
        - String: Model's generated response.
        """
        print("********** Ask Llama 3.2 Lora RAG **********")

        model_inference = FastLanguageModel.for_inference(self.model)
        tokenizer_aux = get_chat_template(self.tokenizer, chat_template="llama-3.1")

        # extract question and options
        question = question_data['question']
        options = [f"{key}: {value}" for key, value in question_data.items() if 'option' in key]

        question_search = (
                f"{question}\n" + " ".join(options) + " "
        )

        # perform rag search using the question to retrieve relevant information
        rag_results = search_rag(question_search, index_file_path=self.faiss_path, chunks_path=self.chunks_path,
                                 top_k=top_k)

        # create the prompt with Chain of Thought (CoT) instructions
        prompt = (
                f"Relevant Information:\n{rag_results}\n"
                f"Question: {question}\n"
                f"Options:\n" +
                "\n".join(options) +
                "\n"
                "Think step by step and choose the correct option.\n"  # "Think step by step and analyze the relevant information carefully, then choose the correct option.\n"
                "You must respond in the format 'correct option: <X>', where <X> is the correct letter for the option."
        )

        # create the input for the model
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer_aux.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to("cuda")

        # generate the response
        outputs = model_inference.generate(
            input_ids=inputs,
            max_new_tokens=self.max_seq_length,
            temperature=0.7,
            min_p=0.9,
            use_cache=True
        )

        # clear memory
        del inputs
        torch.cuda.empty_cache()

        # decode and return the model's output
        return tokenizer_aux.batch_decode(outputs, skip_special_tokens=True)[0]

    def ask_llama_3_2_rag_no_options(self, question_data, top_k=5):
        """
        Function to generate an answer using the model based on the given question and options.

        Parameters:
        - question_data: Dictionary containing the question and options.

        Returns:
        - String: Model's generated response.
        """
        print("**********\nAsk Llama 3.2 Lora RAG no options\n**********")

        model_inference = FastLanguageModel.for_inference(self.model)
        tokenizer_aux = get_chat_template(self.tokenizer, chat_template="llama-3.1")

        # Extract question
        question = question_data['question']

        question_search = (
            f"{question}"
        )

        # Perform RAG search using the question to retrieve relevant information
        rag_results = search_rag(question_search, index_file_path=self.faiss_path, chunks_path=self.chunks_path,
                                 top_k=top_k)

        # Create the prompt with the question and options
        prompt = (
            f"Relevant Information:\n{rag_results}\n"
            f"Question: {question}\n"
            # "Think step by step before answering, analyse and respond with a final answer in the format 'answer: <XXXXX>'."
            "Think step by step before answering and analyze the relevant information carefully, then respond with a final answer in the format 'answer: <XXXXX>'."
        )

        # Create the input for the model
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer_aux.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt",
        ).to("cuda")

        # Generate the response
        outputs = model_inference.generate(
            input_ids=inputs,
            max_new_tokens=self.max_seq_length,
            use_cache=True,
            temperature=0.7,
            min_p=0.9
        )

        # Clear memory
        del inputs  # Remove tensors to free memory
        torch.cuda.empty_cache()  # Release unused memory

        # Decode and return the model's output
        response = tokenizer_aux.batch_decode(outputs, skip_special_tokens=True)[0]
        return response

    def evaluate_questions(self, provided_questions=None, response_file_path='/llama_3.2_lora_short_answer_rag_responses.json'):
        """
        Process all questions and return the model responses.

        Parameters:
        - questions: List of dictionaries containing question data, where each dictionary has:
            - 'question': A string representing the question to be asked to the model.
            - 'answer': A string representing the correct answer format (e.g., 'option 2: PCFICH').
            - 'response': A string that will contain the model's generated response to the question.

        Returns:
        - List: A list of dictionaries where each dictionary contains:
            - 'question': The question as a string.
            - 'answer': The correct answer as a string.
            - 'response': The model's generated response for that question.
        """
        print("**********\nEvaluate Questions\n**********")

        responses = []
        questions_aux = provided_questions if provided_questions is not None else self.questions
        total_questions = len(questions_aux)

        for idx, question_data in enumerate(questions_aux):
            response = self.ask_llama_3_2_lora_rag(question_data)
            responses.append({
                "question": question_data['question'],
                "answer": question_data['answer'],
                "response": response
            })

            # Print progress
            print(f"Responded {idx + 1} of {total_questions} questions...")

        print(responses[0]['response'])

        response_path = response_file_path
        with open(self.output_path + response_path, "w") as json_file:
            json.dump(responses, json_file, indent=4)

        return response_path

    def evaluate_questions_no_options(self):
        """
            Process all questions and return the model responses.

            Parameters:
            - model: The language model loaded for inference.
            - tokenizer: The tokenizer configured with `get_chat_template`.
            - questions: List of dictionaries containing question data, where each dictionary has:
                - 'question': A string representing the question to be asked to the model.
                - 'answer': A string representing the correct answer format (e.g., 'option 2: PCFICH').
                - 'response': A string that will contain the model's generated response to the question.

            Returns:
            - List: A list of dictionaries where each dictionary contains:
                - 'question': The question as a string.
                - 'answer': The correct answer as a string.
                - 'response': The model's generated response for that question.
            """
        print("**********\nEvaluate Questions no options\n**********")

        responses = []
        total_questions = len(self.questions)

        for idx, question_data in enumerate(self.questions):
            response = self.ask_llama_3_2_rag_no_options(question_data)
            responses.append({
                "question": question_data['question'],
                "answer": question_data['answer'],
                "response": response
            })

            # Print progress
            print(f"Responded {idx + 1} of {total_questions} questions...")

        response_path = r"/llama_3.2_lora_short_answer_rag_responses_no_options.json"
        with open(self.output_path + response_path, "w") as json_file:
            json.dump(responses, json_file, indent=4)

        return response_path

    def evaluate_accuracy(self, responses_file_path="/llama_3.2_lora_short_answer_rag_responses.json",
                          first_option=True):
        print("**********\nEvaluate Accuracy\n**********")

        with open(self.output_path + responses_file_path, "r") as file:
            responses = json.load(file)

        print("Responses loaded")

        """
        Evaluate the model's responses and calculate accuracy.
        """
        correct_count = 0  # Track the number of correct responses
        none_count = 0  # Track the number of 'None' responses

        for index, question_data in enumerate(responses):
            evaluation_result = evaluate_model_response(question_data['response'], question_data,
                                                        first_option=first_option)
            options = [f"{key}: {value}" for key, value in self.questions[index].items() if 'option' in key]

            if evaluation_result == 1:
                correct_count += 1  # Increment for correct response
            elif evaluation_result is None:
                # Print only responses that are None
                print("\nWrong Answer")
                print(f"Question {index + 1}: {question_data['question']}")
                print(f"Options:\n" + "\n".join(options) + "\n")
                print(f"Full model response:\n{question_data['response']}")
                print(f"Correct response: {question_data['answer']}")
                print("----------------------------------------------------------------------------------------")
                none_count += 1  # Increment for None response
            else:
                print("\nWrong Answer")
                print(f"Question {index + 1}: {question_data['question']}")
                print(f"Options:\n" + "\n".join(options) + "\n")
                print(f"Model response: {evaluation_result}")
                print(f"Correct response: {question_data['answer']}")
                print("----------------------------------------------------------------------------------------")

        # Calculate and print accuracy
        accuracy = correct_count / len(responses) * 100
        print(f"\nAccuracy: {accuracy:.2f}%")
        print(f"Total 'None' responses: {none_count}")
        print(f"'None' responses means that the model did not give an option")


if __name__ == '__main__':
    print("CUDA Available: ", torch.cuda.is_available())
    print("CUDA Device Name: ", torch.cuda.get_device_name(0))
    torch.cuda.empty_cache()

    # Verify CUDA
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using Device: {device}")

    # call
    output_path_str = "../../files/responses"
    Path(output_path_str).mkdir(parents=True, exist_ok=True)

    inference_rag = InferenceRagLlama(
        model_path="../../models/llama_3.2_1B_FT_lora_4000_questions_short_answer_labels",
        chunks_path="../../files/rag/tspec_chunks_markdown.pkl",
        faiss_path="../../files/rag/faiss_index.bin",
        question_path="../../files/rel17_100_questions.json",
        output_path=output_path_str
    )
    inference_rag.load_questions_mount_model_tokenizer()
    # response_path = inference_rag.evaluate_questions()
    #response_path = inference_rag.evaluate_questions([inference_rag.questions[0]], response_file_path='responses_test.json')
    response_path = inference_rag.evaluate_questions_no_options()
    inference_rag.evaluate_accuracy(response_path, first_option=False)

    # after evaluate questions no options
